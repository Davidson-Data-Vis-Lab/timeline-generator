---
title: "Timelines Analysis"
output: html_document
date: "2024-07-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Timeline Analysis

Analysing the data with statistical tests. **Shapiro Wilk tests for paired differences, and t-test(s) for analysis are found here.**

This file uses code seen in the `timelines_subset_analysis` file. Code here does not give deeper insights into each data subset, rather simply uses the merged datasets for analysis.

Document Outline (as of July 26, 2024):

## General Setup

Packages and ibraries to import, global variables, reading in data file.

```{r echo=FALSE, message=FALSE, results='hide'}
#TODO: Eliminate reundandant packages. 
# (YFA: i added these as i needed. It may be the case that packages are repeated, eg. tidyverse has ggplot2 and dplyr.) 

# import libraries
library(readxl)
library(dplyr)
library(purrr)
library(tidyverse)
library(skimr)
library(ggplot2)
library(ggpubr)
library(RColorBrewer)
library(gridExtra) #fyi: masked from dplyr, to specify function paths #TODO
```

**Importing data file:**

1.  Rename data file with ALL responses data across all 3 groups (EE, EU, EA) as "analysis_data"
2.  Open the data file in Excel and
    1.  remove the second row that contains question text. This is not needed.
    2.  ensure all responses are from 7th July 2024 and onward. Any responses collected before are pilot runs and not true responses.
    3.  Rename the TB question columns to not have the "#1_1" character in them. I am not sure why but this is how Qualtrics exports data for the side by side question type.
        1.  `Find "#1_1"` (find all) and then `Replace with ""` (replace all).

```{r echo=FALSE}

# Reading data from data file
pilotDF <- read_excel("./data/analysis_data.xlsx")

#cleaning data (exclude unfinished responses)
pilotDF <- pilotDF %>%
  filter(pilotDF$Finished != "False")

# Defining global variables
MAX_SCORE <- 12
SCORE_VALUE <- 1

#variables subject to change as we get more responses:

#no. of total respondants
SAMPLE_SIZE <- nrow(pilotDF) 

#no. of responses that answer with English as primary lang
eng_primary_size <- pilotDF %>%
  filter(pilotDF$`CF EE` == "I AGREE") %>%
  nrow() 

#no. of responses that answer with Urdu as primary lang:
urdu_primary_size <- pilotDF %>%
  filter(pilotDF$`EU CF` == "I AGREE") %>%
  nrow() 

#no. of responses that answer with Arabic as primary lang
arb_primary_size <- pilotDF %>%
  filter(pilotDF$`EA CF` == "I AGREE") %>%
  nrow() 

# printing out values for html/ pdf
paste0("Total Sample Size: ", SAMPLE_SIZE)
paste0("English(primary) pool size: ", eng_primary_size)
paste0("Urdu pool size: ", urdu_primary_size)
paste0("Arabic pool size: ", arb_primary_size)
```

## Defining Functions/ Procedures

### Summing Accuracies

Sums up accuracy scores for provided language and orientation group (prefix) from dataset.

Maximum possible score for any condition is 12. Calculates weighed accuracy (achieved score divided by 12) for provided prefix and for each participant.

\*Weighed accuracy approach to account for negative marking of the questions (-1, -2, -5, -10).

```{r}
# Function to process accuracy data
# RETURNS accuracy percentage (scored/12) for lanuage+orientation combo
acc_sum <- function(df, prefix) {
  
  # Select specific columns
  selected_columns <- df %>%
    select(matches(paste0("^", prefix, " - Q[1-6]$")))
  
  # Converting character vectors to numeric
  data_numeric <- lapply(selected_columns, function(column) as.numeric(column))
  
  # Combine the lists into a data frame to facilitate row-wise operations
  data_numeric_df <- as.data.frame(data_numeric)
  # this creates a data frame for the specified language-timeline (prefix) combo.
  # will end up creating 9 data frames (3 per language, TB, RL, LR)
  # each data frame will have [SAMPLE_SIZE] rows * 12 columns (questions)
  
  # Filter out rows where all values are NA #
  data_filtered <- data_numeric_df[rowSums(is.na(data_numeric_df)) != ncol(data_numeric_df), ]
  # the no. of rows for the data frame will match the pool size for the specified langugae participants
  # eg. Urdu speakers df will have [urdu_primary_size] rows by 12 columns
  

  # Replace negative values with zero 
  data_filtered[data_filtered < 0] <- 0
  # for current analysis purposes: not interested in analysing performance by quest. difficulty
  
  # List to store sums of each index across columns
  sums <- numeric(nrow(data_filtered))
  
  # Calculate and store sums of each index across columns
  for (i in seq_along(sums)) {
    sums[i] <- sum(data_filtered[i, ], na.rm = TRUE)
  }
  #sums can range bw 0 to 12
  #print(sums) #debugging
  
  #accuracy for each participant
  accuracy_raw <- sums / MAX_SCORE
  
  # return accuracy + raw scores
  return(list(accuracy = accuracy_raw, sums = sums))
}
```

### Summing Times

Sums up time spent on questions in a provided language and orientation group (prefix) from dataset.

Sums up time taken to complete the 12 questions in the given prefix for each participant.

```{r}
# Function to process time data
time_sum <- function(df, prefix) {
  
  # Select specific columns
  selected_columns <- df %>%
    select(matches(paste0("^", prefix, " - Q[1-6] Time_Page Submit$")))
  
  # Converting character vectors to numeric
  data_numeric <- lapply(selected_columns, function(column) as.numeric(column))
  
  # Combine the lists into a data frame to facilitate row-wise operations
  data_numeric_df <- as.data.frame(data_numeric)
  
  # Filter out rows where all values are NA
  data_filtered <- data_numeric_df[rowSums(is.na(data_numeric_df)) != ncol(data_numeric_df), ]
  
  # List to store sums of each index across columns
  sums <- numeric(nrow(data_filtered))
  
  # Calculate and store sums of each index across columns
  for (i in seq_along(sums)) {
    sums[i] <- sum(data_filtered[i, ], na.rm = TRUE)
  }
  
  return(sums)
}
```

### Create and Merge Data Subsets

Code from `timeline_subsets_analysis`, but improved to minimize redundancy.

*Note on AI Usage:* ChatGPT was used for the code chunk below in order to place code seen in the other file as functions. Adjustments and further refinements (such as creating a helper function, logic to include columns 3 and 4, etc.) was added to AI results.

```{r}
#GPT used
#helper to subset_merger
#n: sample/ pool size
#lang: initial of lanuage used (E, A, U)
#or: orientation abbreviations (LR, RL, TB)
#cols: timeline numbers of interest (either 1,2 or 3,4)
subset_creator <- function(n, lang, or, cols){
  
  participant_ID <- rep(1:n, times=2)
  
  # Construct the dynamic string for the acc_sum and time_sum functions
  dynamic_string <- paste0(lang, or, cols)
  
  # Calculating weighted accuracy
  acc_value <- acc_sum(pilotDF, dynamic_string)
  #shapiro_test_acc <- shapiro.test(acc_value)
  
  # Summing times
  time_value <- time_sum(pilotDF, dynamic_string)
  #shapiro_test_time <- shapiro.test(time_value)
  
  # Creating data frame for plot
  data_df <- data.frame(
    p_id = participant_ID,
    metric = rep(c("time", "accuracy"), each=n),
    value = c(time_value, acc_value$accuracy),
    score = acc_value$sums
  )
  
  return(data_df)
  
  #return(list(
    #data_df = data_df,
    #shapiro_test_acc = shapiro_test_acc,
    #shapiro_test_time = shapiro_test_time
  # ))
}

#merge datasets
subset_merger <- function(pool_size, language, use12=TRUE){
  
  # Set lanuage initial based on the language parameter
  if (language == "English") {
    LI <- "E"
  } else if (language == "Urdu") {
    LI <- "U"
  } else if(language == "Arabic") {
    LI <- "A"
  }
  
  #specify columns (12) or (34)
  if(use12){
    cols <- "[12]"
  }
  else{
    cols <- "[34]"
  }
  
  # Extract data frames from the results
  LR_df <- subset_creator(pool_size, LI, "LR", cols)
  RL_df <- subset_creator(pool_size, LI, "RL", cols)
  TB_df <- subset_creator(pool_size, LI, "TB", cols)
  
  # Merge the data frames
  df_lang <- LR_df %>%
    rename(value_LR = value) %>%
    full_join(RL_df %>% rename(value_RL = value), by = c("p_id", "metric")) %>%
    full_join(TB_df %>% rename(value_TB = value), by = c("p_id", "metric"))
  
  # Sum the score columns and create the total_score column
  df_lang <- df_lang %>%
    rowwise() %>%
    mutate(total_score = sum(c_across(starts_with("score")), na.rm = TRUE))
  
  return(df_lang)
}

```

## Creating Datasets for Use

```{r}
#English (Universal/ secondary language)
df_ENG_ALL <- subset_merger(SAMPLE_SIZE, "English")
head(df_ENG_ALL)

#English (as primary languge) 
#false to signify using ENG34 columns (instead of ENG12 which are universal quests.)
df_ENG34 <- subset_merger(eng_primary_size, "English", FALSE)
head(df_ENG34)

#Urdu 
df_URDU <- subset_merger(urdu_primary_size, "Urdu")
head(df_URDU)

#Arabic
df_ARB <- subset_merger(arb_primary_size, "Arabic")
head(df_ARB)
```

## Test: Significant Differences between Accuracy for Orientations?

Paired two tailed t-test, for all language groups (E,U, or A) [excludes English universal questions].

Pairs:

-   \<lang\> LR vs. \<lang\> RL

-   \<lang\> RL vs. \<lang\> TB

-   \<lang\> TB vs. \<lang\> LR

### English (Primary Language)

**t-test: ELR vs. ERL accuracy**

**Null Hyp. :** There is no significant difference in the accuracy scores between a LR orientation and a RL orientation.

**Alternative Hyp. :** There is a significant difference in the accuracy scores between a LR orientation and a RL orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 
#compute differences:
diffs <- df_ENG34$value_LR[df_ENG34$metric == "accuracy"] - df_ENG34$value_RL[df_ENG34$metric == "accuracy"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.

#t-test: ELR vs. ERL accuracy

test_elr_erl_acc <- t.test(
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_LR),
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_RL),
  paired = TRUE
)

print(test_elr_erl_acc)
```

**t-test: ETB vs. ELR accuracy**

**Null Hyp. :** There is no significant difference in the accuracy scores between a LR orientation and a TB orientation.

**Alternative Hyp. :** There is a significant difference in the accuracy scores between a LR orientation and a TB orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 
#compute differences:
diffs <- df_ENG34$value_TB[df_ENG34$metric == "accuracy"] - df_ENG34$value_LR[df_ENG34$metric == "accuracy"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.

#t-test: ETB vs. ELR accuracy

test_etb_elr_acc <- t.test(
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_LR),
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_TB),
  paired = TRUE
)

print(test_etb_elr_acc)
```

**t-test: ERL vs. ETB accuracy**

**Null Hyp. :** There is no significant difference in the accuracy scores between a TB orientation and a RL orientation.

**Alternative Hyp. :** There is a significant difference in the accuracy scores between a TB orientation and a RL orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 
#compute differences:
diffs <- df_ENG34$value_RL[df_ENG34$metric == "accuracy"] - df_ENG34$value_TB[df_ENG34$metric == "accuracy"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.

#t-test: ERL vs. ETB accuracy

test_erl_etb_acc <- t.test(
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_RL),
  df_ENG34 %>% filter(metric=="accuracy") %>% pull(value_TB),
  paired = TRUE
)

print(test_erl_etb_acc)
```

## Test: Significant Differences between Time Taken for Orientations?

***What ORIENTATION is best for \<LANGUAGE\>?***

Paired two tailed t-test, for all language groups (E,U, or A) [excludes English universal questions].

Pairs:

-   \<lang\> LR vs. \<lang\> RL

-   \<lang\> RL vs. \<lang\> TB

-   \<lang\> TB vs. \<lang\> LR

### English (Primary Language)

**t-test: ELR vs. ERL time**

**Null Hyp. :** There is no significant difference in the time taken to complete tasks between a LR orientation and a RL orientation.

**Alternative Hyp. :** There is a significant difference in the time taken to complete tasks between a LR orientation and a RL orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 

#compute differences:
diffs <- df_ENG34$value_LR[df_ENG34$metric == "time"] - df_ENG34$value_RL[df_ENG34$metric == "time"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.


#t-test: ELR vs. ERL time

test_elr_erl_time <- t.test(
  df_ENG34 %>% filter(metric=="time") %>% pull(value_LR),
  df_ENG34 %>% filter(metric=="time") %>% pull(value_RL),
  paired = TRUE
)

print(test_elr_erl_time)
```

**t-test: ETB vs. ELR time**

**Null Hyp. :** There is no significant difference in the time taken to complete tasks between a TB orientation and a LR orientation.

**Alternative Hyp. :** There is a significant difference in the time taken to complete tasks between a TB orientation and a LR orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 

#compute differences:
diffs <- df_ENG34$value_TB[df_ENG34$metric == "time"] - df_ENG34$value_LR[df_ENG34$metric == "time"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.

#t-test: ETB vs. ELR time

test_etb_elr_time <- t.test(
  df_ENG34 %>% filter(metric=="time") %>% pull(value_LR),
  df_ENG34 %>% filter(metric=="time") %>% pull(value_TB),
  paired = TRUE
)

print(test_etb_elr_time)
```

**t-test: ERL vs. ETB time**

**Null Hyp. :** There is no significant difference in the time taken to complete tasks between a TB orientation and a RL orientation.

**Alternative Hyp. :** There is a significant difference in the time taken to complete tasks between a TB orientation and a RL orientation.

*Assumed significance level/ alpha: 0.05*

```{r echo=FALSE}
# are the paired differences normal? 

#compute differences:
diffs <- df_ENG34$value_RL[df_ENG34$metric == "time"] - df_ENG34$value_TB[df_ENG34$metric == "time"]

#print(diffs) #debugging

#shapiro wilk to test normality for paired differences:
shapiro.test(diffs) #p-value > alpha to confirm normality.

#t-test: ERL vs. ETB time

test_erl_etb_time <- t.test(
  df_ENG34 %>% filter(metric=="time") %>% pull(value_RL),
  df_ENG34 %>% filter(metric=="time") %>% pull(value_TB),
  paired = TRUE
)

print(test_erl_etb_time)
```

## Test: Responses Different than Random Chance?

Collecting group scores:

```{r}
# collecting English scores for respondents who answered in Eng-Eng
eng34_scores <- pilotDF %>%
  filter(`CF EE` == "I AGREE") %>%
  pull(SC0)

# collecting urdu scores for respondents who answered in Eng-Urdu
urdu_scores <- pilotDF %>%
  filter(`EU CF` == "I AGREE") %>%
  pull(SC0)

# collecting arabic scores for respondents who answered in Eng-Urdu
arb_scores <- pilotDF %>%
  filter(`EA CF` == "I AGREE") %>%
  pull(SC0)
```

**One tailed t-test (upper-tailed).**

Threshold: 28% accuracy (score: 20/72) (based on the fact that for each timeline, participants choose between 21 choices in total)

**H0: The mean accuracy is less than or equal to the theoretical accuracy (28%).**

**H1: The mean accuracy is greater than the the theoretical accuracy (28%).**

Alpha: 0.05

assuming normality of data.

**English English**

```{r}
# Perform the one-tailed t-test for ENG-ENG
# H0: Mean accuracy <= 20 (chance level)
# H1: Mean accuracy > 20 (better than chance)

t_test_result <- t.test(eng34_scores, mu = 20, alternative = "greater")

# View the results
print(t_test_result)
```

**English Urdu**

as of July 26: n \< 30, should implement normality checks

```{r}
# normality?
shapiro.test(urdu_scores)

# Perform the one-tailed t-test for ENG-ENG
# H0: Mean accuracy <= 20 (chance level)
# H1: Mean accuracy > 20 (better than chance)

t_test_result <- t.test(urdu_scores, mu = 20, alternative = "greater")

# View the results
print(t_test_result)
```

**English Arabic**

as of July 26: n \< 30, should implement normality checks

```{r}
# normality?
shapiro.test(arb_scores)

# Perform the one-tailed t-test for ENG-ENG
# H0: Mean accuracy <= 20 (chance level)
# H1: Mean accuracy > 20 (better than chance)

t_test_result <- t.test(arb_scores, mu = 20, alternative = "greater")

# View the results
print(t_test_result)
```

## (not doing as of July 26) Unpaired t-tests - TIME

ASSUMING NORMALITY OF ALL DATA.

**ASSUMES EQUAL VARIANCE - FTEST NEEDED.**

F-TEST also assumes normality.

***What \<LANG\> is best for \<ORIENTATION\> (LR, RL, TB)?***

Pairs:

-   For all 3 orientations: (9 tests)

    -   Urdu vs Eng

    -   Eng vs Arb

    -   Arb vs. Urdu
